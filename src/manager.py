import csv
import conf
import fb
from my_scrapers.the_student_room.spiders.the_student_room_spider import tsr_source
from sources import TweetStreamerSource
import nlp
import models
from db import MyDB
import sys
from pipes import Source, Processor, Writer, Join
import psycopg2.extras
from nltk.corpus import stopwords
import math

def recreate_tables(db, drop=False):
    '''
    Recreates the database tables according to the schema if they dont exist

    :param db: the connection to the database that neededs it's tables recreated
    :param drop: default `False`, deletes the tables if they already exist
    '''

    if drop:
        db.drop_table("schools")
        db.drop_table("post")
        db.drop_table("topic")

    db.create_table(
            "schools",
            {
                "establishment_name": "text PRIMARY KEY",
                "URN": "integer NOT NULL",
                "postcode": "text NOT NULL",
                "type_of_establishment": "text NOT NULL",
                "phase_of_education": "text NOT NULL"
                }
    )

    db.create_table(
            "topic",
            {
                "id": "int GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY",
                "topic_description": "text"
                }
            )

    db.create_table(
            "post",
            {
                "post_id": "bigint PRIMARY KEY",
                "timestamp": "timestamp NOT NULL",
                "raw_text": "text", "sentiment": "real NOT NULL",
                "school_id": "text references schools(establishment_name)",
                "topic_id": "integer references topic(id)"
                }
            )

def load_schools_table(db):
    '''
    Loads a list of schools from `file('data/schools.csv')` into a database using the database connection `db`

    :param db: the connection to the database to be loaded
    '''
    data_table = csv.DictReader(open("data/schools.csv"))
    for row in data_table:
        db.add_row("schools", {"URN": row["URN"] , "establishment_name": row["EstablishmentName"], "postcode": row["Postcode"], "type_of_establishment": row["TypeOfEstablishment (name)"], "phase_of_education": row["PhaseOfEducation (name)"]})

def load_fb_posts(db):
    classifier = nlp.ner_classifier(["University College London"])
    src = Source([
                    Processor(
                            [Processor([
                                Writer(lambda x: db.add_row("post", x))],
                                classifier
                                )],
                            nlp.sentiment())],
                    fb.run
                )
    src.start()

def retopic(db):
    '''
        Computes the topics for all of the posts in `db`
        :param db: the database to recompute topics for
    '''
    import topics
    
    schools = db.select("select distinct(school_id) from post where topic_id is null")
    
    for school in schools:
        school = school["school_id"]
        topic_batches = db.select("select distinct(to_char(timestamp, 'YYYY-MM')) from post order by to_char(timestamp, 'YYYY-MM')")
        for topic_batch in topic_batches:
            topic_batch = topic_batch["to_char"]
            unassigned_posts = db.select(
                    "select * from post where "
                    "     topic_id is null "
                    " and to_char(timestamp, 'YYYY-MM') = %s "
                    " and school_id = %s", topic_batch, school)

            if unassigned_posts == []:
                continue

            for i, topic in enumerate(topics.split_raw(
                    unassigned_posts,
                    n=math.ceil(len(unassigned_posts) / 5.0))):
                if topic:
                    text = school + "::" + topic_batch + "::" + str(i) #topic[0]["raw_text"][:125] + "..."
                    topic_id = db.add_row("topic", {"topic_description": text}, returning="id")[0]
                    for post in topic:
                        db.update_row("post", post["post_id"], {"topic_id": topic_id}, id_field="post_id")


def data_pipeline(db):
    '''
    Constructs the compute graph for the data processing pipline and set's it running. Warning: Blocks the main thread
    :param db: the database to recompute insert the posts into
    '''
    debug = Writer(print)
    out = Writer(lambda x: db.add_row("post", x))
    sentiment = Processor([out, debug], nlp.sentiment())
    twitter = Source([sentiment], TweetStreamerSource())
    tsr = Source([sentiment], tsr_source(conf.tsr_ner_shortcut))


def main():
    db = MyDB("dbname=capita user=amartya password=test")
    if sys.argv[1] == "load_schools":
        load_schools_table(db)
    elif sys.argv[1] == "recreate_tables":
        recreate_tables(db)
    elif sys.argv[1] == "load_fb":
        load_fb_posts(db)
    elif sys.argv[1] == "retopic":
        retopic(db)
    elif sys.argv[1] == "data_pipeline":
        data_pipeline(db)
    else:
        print("error invalid command")


if __name__ == '__main__':
    main()
