import csv
import conf
import fb
from my_scrapers.the_student_room.spiders.the_student_room_spider import tsr_source
from sources import TweetStreamerSource
import nlp
import models
from db import MyDB
import sys
from pipes import Source, Processor, Writer, Join
import psycopg2.extras
from nltk.corpus import stopwords

def recreate_tables(db, drop=False):
    if drop:
        db.drop_table("schools")
        db.drop_table("post")
        db.drop_table("topic")

    db.create_table(
            "schools",
            {
                "establishment_name": "text PRIMARY KEY",
                "URN": "integer NOT NULL",
                "postcode": "text NOT NULL",
                "type_of_establishment": "text NOT NULL",
                "phase_of_education": "text NOT NULL"
                }
    )

    db.create_table(
            "topic",
            {
                "id": "int GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY",
                "topic_description": "text"
                }
            )

    db.create_table(
            "post",
            {
                "post_id": "bigint PRIMARY KEY",
                "timestamp": "timestamp NOT NULL",
                "raw_text": "text", "sentiment": "real NOT NULL",
                "school_id": "text references schools(establishment_name)",
                "topic_id": "integer references topic(id)"
                }
            )

def load_schools_table(db):
    data_table = csv.DictReader(open("data/schools.csv"))
    for row in data_table:
        db.add_row("schools", {"URN": row["URN"] , "establishment_name": row["EstablishmentName"], "postcode": row["Postcode"], "type_of_establishment": row["TypeOfEstablishment (name)"], "phase_of_education": row["PhaseOfEducation (name)"]})

def load_fb_posts(db):
    classifier = nlp.ner_classifier(["University College London"])
    src = Source([
                    Processor(
                            [Processor([
                                Writer(lambda x: db.add_row("post", x))],
                                classifier
                                )],
                            nlp.sentiment())],
                    fb.run
                )
    src.start()

def retopic(db):
    import topics
    unassigned_posts = db.select("select * from post where topic_id is null")
    for topic in topics.split_raw(unassigned_posts, n=10):
        if topic:
            text = topic[0]["raw_text"][:125] + "..."
            topic_id = db.add_row("topic", {"topic_description": text}, returning="id")[0]
            for post in topic:
                db.update_row("post", post["post_id"], {"topic_id": topic_id}, id_field="post_id")


def data_pipeline(db, schools=[], terms=[]):
    debug = Writer(print)
    out = Writer(lambda x: db.add_row("post", x))
    #join = Join([out], lambda d: d["raw_text"], count=1)
    #ner_debug = Writer(lambda x: print("NER:", x))
    #ner = Processor([join, out], nlp.fuzzy_classifier(schools))
    sentiment = Processor([out, debug], nlp.sentiment())
    twitter = Source([sentiment], TweetStreamerSource(terms))
    tsr = Source([sentiment], tsr_source(conf.tsr_ner_shortcut))


def main():
    db = MyDB("dbname=capita user=amartya password=test")
    if sys.argv[1] == "load_schools":
        load_schools_table(db)
    elif sys.argv[1] == "recreate_tables":
        recreate_tables(db)
    elif sys.argv[1] == "load_fb":
        load_fb_posts(db)
    elif sys.argv[1] == "retopic":
        retopic(db)
    elif sys.argv[1] == "data_pipeline":
        data_pipeline(db)
    else:
        print("error invalid command")


if __name__ == '__main__':
    main()
