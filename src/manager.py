import csv
import conf
import fb
from my_scrapers.the_student_room.spiders.the_student_room_spider import tsr_source
from sources import TweetStreamerSource
import nlp
import models
from db import MyDB
import sys
from pipes import Source, Processor, Writer, Join
import psycopg2.extras
from nltk.corpus import stopwords
import math

def recreate_tables(db, drop=False):
    if drop:
        db.drop_table("schools")
        db.drop_table("post")
        db.drop_table("topic")

    db.create_table(
            "schools",
            {
                "establishment_name": "text PRIMARY KEY",
                "URN": "integer NOT NULL",
                "postcode": "text NOT NULL",
                "type_of_establishment": "text NOT NULL",
                "phase_of_education": "text NOT NULL"
                }
    )

    db.create_table(
            "topic",
            {
                "id": "int GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY",
                "topic_description": "text"
                }
            )

    db.create_table(
            "post",
            {
                "post_id": "bigint PRIMARY KEY",
                "timestamp": "timestamp NOT NULL",
                "raw_text": "text", "sentiment": "real NOT NULL",
                "school_id": "text references schools(establishment_name)",
                "topic_id": "integer references topic(id)"
                }
            )

def load_schools_table(db):
    data_table = csv.DictReader(open("data/schools.csv"))
    for row in data_table:
        db.add_row("schools", {"URN": row["URN"] , "establishment_name": row["EstablishmentName"], "postcode": row["Postcode"], "type_of_establishment": row["TypeOfEstablishment (name)"], "phase_of_education": row["PhaseOfEducation (name)"]})

def load_fb_posts(db):
    classifier = nlp.ner_classifier(["University College London"])
    src = Source([
                    Processor(
                            [Processor([
                                Writer(lambda x: db.add_row("post", x))],
                                classifier
                                )],
                            nlp.sentiment())],
                    fb.run
                )
    src.start()

def retopic(db):
    print("TEST")
    import topics
    print("TEST")
    
    schools = db.select("select distinct(school_id) from post where topic_id is null")
    
    for school in schools:
        school = school["school_id"]
        topic_batches = db.select("select distinct(to_char(timestamp, 'YYYY-MM')) from post order by to_char(timestamp, 'YYYY-MM')")
        for topic_batch in topic_batches:
            print(school, topic_batch)
            topic_batch = topic_batch["to_char"]
            unassigned_posts = db.select(
                    "select * from post where "
                    "     topic_id is null "
                    " and to_char(timestamp, 'YYYY-MM') = %s "
                    " and school_id = %s", topic_batch, school)

            if unassigned_posts == []:
                continue

            print(unassigned_posts)
            for i, topic in enumerate(topics.split_raw(
                    unassigned_posts,
                    n=math.ceil(len(unassigned_posts) / 5.0))):
                print(topic)
                if topic:
                    text = school + "::" + topic_batch + "::" + str(i) #topic[0]["raw_text"][:125] + "..."
                    topic_id = db.add_row("topic", {"topic_description": text}, returning="id")[0]
                    for post in topic:
                        db.update_row("post", post["post_id"], {"topic_id": topic_id}, id_field="post_id")


def data_pipeline(db, schools=[], terms=[]):
    debug = Writer(print)
    out = Writer(lambda x: db.add_row("post", x))
    #join = Join([out], lambda d: d["raw_text"], count=1)
    #ner_debug = Writer(lambda x: print("NER:", x))
    #ner = Processor([join, out], nlp.fuzzy_classifier(schools))
    sentiment = Processor([out, debug], nlp.sentiment())
    twitter = Source([sentiment], TweetStreamerSource(terms))
    tsr = Source([sentiment], tsr_source(conf.tsr_ner_shortcut))


def main():
    db = MyDB("dbname=capita user=amartya password=test")
    if sys.argv[1] == "load_schools":
        load_schools_table(db)
    elif sys.argv[1] == "recreate_tables":
        recreate_tables(db)
    elif sys.argv[1] == "load_fb":
        load_fb_posts(db)
    elif sys.argv[1] == "retopic":
        retopic(db)
    elif sys.argv[1] == "data_pipeline":
        data_pipeline(db)
    else:
        print("error invalid command")


if __name__ == '__main__':
    main()
